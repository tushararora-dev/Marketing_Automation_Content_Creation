{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f41e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import trafilatura\n",
    "import re\n",
    "def get_website_text_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text content from website using trafilatura\n",
    "    \n",
    "    Args:\n",
    "        url: Website URL\n",
    "    \n",
    "    Returns:\n",
    "        Clean text content from the website\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Download the webpage\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        \n",
    "        if not downloaded:\n",
    "            raise Exception(f\"Failed to download content from {url}\")\n",
    "        \n",
    "        # Extract main text content\n",
    "        text_content = trafilatura.extract(downloaded)\n",
    "        \n",
    "        if not text_content:\n",
    "            raise Exception(f\"Failed to extract text from {url}\")\n",
    "        \n",
    "        return text_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback: try basic requests\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            })\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Basic text extraction from HTML\n",
    "                html_content = response.text\n",
    "                # Remove HTML tags (basic approach)\n",
    "                text_content = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "                # Clean up whitespace\n",
    "                text_content = ' '.join(text_content.split())\n",
    "                return text_content[:5000]  # Limit to first 5000 chars\n",
    "            else:\n",
    "                raise Exception(f\"HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as fallback_error:\n",
    "            raise Exception(f\"All extraction methods failed: {str(e)}, {str(fallback_error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2713adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing GPT-5\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
      "Learn moreWhat can I help with?\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
      "Learn more\n"
     ]
    }
   ],
   "source": [
    "print(get_website_text_content(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb17867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def validate_url(url: str) -> bool:\n",
    "    \"\"\"Validate if URL is accessible\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.head(url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "print(validate_url(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfa1ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'OpenAI', 'description': 'We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Building safe and beneficial AGI is our mission.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def get_website_metadata(url: str) -> Dict[str, str]:\n",
    "    \"\"\"Get website metadata (title, description, etc.)\"\"\"\n",
    "    \n",
    "    metadata = {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            \n",
    "            # Extract title\n",
    "            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)\n",
    "            if title_match:\n",
    "                metadata['title'] = title_match.group(1).strip()\n",
    "            \n",
    "            # Extract meta description\n",
    "            desc_match = re.search(r'<meta[^>]*name=[\"\\']description[\"\\'][^>]*content=[\"\\']([^\"\\']+)[\"\\']', html_content, re.IGNORECASE)\n",
    "            if desc_match:\n",
    "                metadata['description'] = desc_match.group(1).strip()\n",
    "            \n",
    "            # Extract meta keywords\n",
    "            keywords_match = re.search(r'<meta[^>]*name=[\"\\']keywords[\"\\'][^>]*content=[\"\\']([^\"\\']+)[\"\\']', html_content, re.IGNORECASE)\n",
    "            if keywords_match:\n",
    "                metadata['keywords'] = keywords_match.group(1).strip()\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return metadata\n",
    "print(get_website_metadata(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c578b6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'twitter': 'https://twitter.com/OpenAI', 'instagram': 'https://instagram.com/openai', 'linkedin': 'https://linkedin.com/openai', 'tiktok': 'https://tiktok.com/openai'}\n"
     ]
    }
   ],
   "source": [
    "def extract_social_links(url: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract social media links from website\"\"\"\n",
    "    \n",
    "    social_links = {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            \n",
    "            # Social media patterns\n",
    "            social_patterns = {\n",
    "                'facebook': r'(?:facebook\\.com/|fb\\.com/)([A-Za-z0-9\\.]+)',\n",
    "                'twitter': r'(?:twitter\\.com/|x\\.com/)([A-Za-z0-9_]+)',\n",
    "                'instagram': r'instagram\\.com/([A-Za-z0-9_.]+)',\n",
    "                'linkedin': r'linkedin\\.com/(?:company/|in/)([A-Za-z0-9-]+)',\n",
    "                'youtube': r'youtube\\.com/(?:channel/|user/|c/)([A-Za-z0-9_-]+)',\n",
    "                'tiktok': r'tiktok\\.com/@([A-Za-z0-9_.]+)'\n",
    "            }\n",
    "            \n",
    "            for platform, pattern in social_patterns.items():\n",
    "                matches = re.findall(pattern, html_content, re.IGNORECASE)\n",
    "                if matches:\n",
    "                    social_links[platform] = f\"https://{platform}.com/{matches[0]}\"\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return social_links\n",
    "\n",
    "print(extract_social_links(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a0ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#007bff', '#6c757d']\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "def extract_brand_colors(url: str) -> List[str]:\n",
    "    \"\"\"Extract brand colors from website (basic implementation)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            \n",
    "            # Look for CSS color definitions\n",
    "            color_patterns = [\n",
    "                r'color[:\\s]*([#][0-9a-fA-F]{6})',\n",
    "                r'background-color[:\\s]*([#][0-9a-fA-F]{6})',\n",
    "                r'border-color[:\\s]*([#][0-9a-fA-F]{6})'\n",
    "            ]\n",
    "            \n",
    "            colors = set()\n",
    "            for pattern in color_patterns:\n",
    "                matches = re.findall(pattern, html_content)\n",
    "                colors.update(matches)\n",
    "            \n",
    "            # Return common brand colors (excluding common web colors)\n",
    "            exclude_colors = {'#ffffff', '#000000', '#f0f0f0', '#e0e0e0'}\n",
    "            brand_colors = [color for color in colors if color.lower() not in exclude_colors]\n",
    "            \n",
    "            return brand_colors[:5] if brand_colors else ['#007bff', '#6c757d']\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Default color scheme\n",
    "    return ['#007bff', '#6c757d']\n",
    "\n",
    "print(extract_brand_colors(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a141d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing GPT-5\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
      "Learn moreWhat can I help with?\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
      "Learn more\n"
     ]
    }
   ],
   "source": [
    "def get_website_text_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean text content from website using trafilatura\n",
    "    \n",
    "    Args:\n",
    "        url: Website URL\n",
    "    \n",
    "    Returns:\n",
    "        Clean text content from the website\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Download the webpage\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        \n",
    "        if not downloaded:\n",
    "            raise Exception(f\"Failed to download content from {url}\")\n",
    "        \n",
    "        # Extract main text content\n",
    "        text_content = trafilatura.extract(downloaded)\n",
    "        \n",
    "        if not text_content:\n",
    "            raise Exception(f\"Failed to extract text from {url}\")\n",
    "        \n",
    "        return text_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback: try basic requests\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            })\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Basic text extraction from HTML\n",
    "                html_content = response.text\n",
    "                # Remove HTML tags (basic approach)\n",
    "                text_content = re.sub(r'<[^>]+>', ' ', html_content)\n",
    "                # Clean up whitespace\n",
    "                text_content = ' '.join(text_content.split())\n",
    "                return text_content[:5000]  # Limit to first 5000 chars\n",
    "            else:\n",
    "                raise Exception(f\"HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as fallback_error:\n",
    "            raise Exception(f\"All extraction methods failed: {str(e)}, {str(fallback_error)}\")\n",
    "    \n",
    "print(get_website_text_content(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173e478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai\n"
     ]
    }
   ],
   "source": [
    "def extract_brand_name(content: str, url: str) -> str:\n",
    "    \"\"\"Extract brand name from website content\"\"\"\n",
    "    \n",
    "    # Look for common brand name patterns\n",
    "    patterns = [\n",
    "        r'<title[^>]*>([^<]+)</title>',\n",
    "        r'brand[:\\s]+([A-Za-z0-9\\s]+)',\n",
    "        r'company[:\\s]+([A-Za-z0-9\\s]+)',\n",
    "        r'welcome to ([A-Za-z0-9\\s]+)',\n",
    "        r'about ([A-Za-z0-9\\s]+)'\n",
    "    ]\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE)\n",
    "        if matches:\n",
    "            # Clean and return first match\n",
    "            brand_name = matches[0].strip()\n",
    "            # Remove common words\n",
    "            brand_name = re.sub(r'\\b(the|inc|llc|ltd|company|corp|corporation)\\b', '', brand_name, flags=re.IGNORECASE)\n",
    "            brand_name = brand_name.strip()\n",
    "            if len(brand_name) > 2 and len(brand_name) < 50:\n",
    "                return brand_name.title()\n",
    "    \n",
    "    # Fallback: extract from domain\n",
    "    return extract_domain_name(url)\n",
    "\n",
    "content = \"\"\"Introducing GPT-5\n",
    "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
    "Learn moreWhat can I help with?\n",
    "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
    "Learn more\"\"\"\n",
    "url = \"https://openai.com\"\n",
    "print(extract_brand_name(content, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c536fa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "def extract_domain_name(url: str) -> str:\n",
    "    \"\"\"Extract clean domain name as brand name fallback\"\"\"\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        \n",
    "        # Remove www and common extensions\n",
    "        domain = re.sub(r'^www\\.', '', domain)\n",
    "        domain = re.sub(r'\\.(com|org|net|io|co|us|uk|ca)$', '', domain, flags=re.IGNORECASE)\n",
    "        \n",
    "        return domain.title()\n",
    "        \n",
    "    except:\n",
    "        return \"Brand\"\n",
    "\n",
    "print(extract_domain_name(\"https://openai.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b09cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn moreWhat can I help with?\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n"
     ]
    }
   ],
   "source": [
    "def extract_brand_description(content: str) -> str:\n",
    "    \"\"\"Extract brand description or mission statement\"\"\"\n",
    "    \n",
    "    # Look for description patterns\n",
    "    patterns = [\n",
    "        r'(?:we are|we\\'re|our mission|about us|description)[:\\s]+([^.!?]{20,200}[.!?])',\n",
    "        r'(?:providing|offering|specializing in|focused on)[:\\s]+([^.!?]{20,200}[.!?])',\n",
    "        r'(?:helping|enabling|empowering)[^.!?]*?([^.!?]{20,200}[.!?])'\n",
    "    ]\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE | re.DOTALL)\n",
    "        if matches:\n",
    "            description = matches[0].strip()\n",
    "            # Clean up the description\n",
    "            description = re.sub(r'\\s+', ' ', description)\n",
    "            if len(description) > 20:\n",
    "                return description.capitalize()\n",
    "    \n",
    "    # Fallback: extract first meaningful paragraph\n",
    "    sentences = content.split('.')\n",
    "    for sentence in sentences[:10]:\n",
    "        sentence = sentence.strip()\n",
    "        if len(sentence) > 50 and len(sentence) < 300:\n",
    "            # Check if it's descriptive (contains certain keywords)\n",
    "            if any(word in sentence.lower() for word in ['provide', 'offer', 'help', 'solution', 'service', 'product']):\n",
    "                return sentence + '.'\n",
    "    \n",
    "    return \"A leading provider of quality products and services.\"\n",
    "\n",
    "print(extract_brand_description(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6f8854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Products and Services']\n"
     ]
    }
   ],
   "source": [
    "def extract_products(content: str) -> List[str]:\n",
    "    \"\"\"Extract product or service names from content\"\"\"\n",
    "    \n",
    "    products = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Look for product/service patterns\n",
    "    product_patterns = [\n",
    "        r'(?:products?|services?|offering|solutions?)[:\\s]+([^.!?]{10,100})',\n",
    "        r'(?:we offer|we provide|available)[:\\s]+([^.!?]{10,100})',\n",
    "        r'(?:including|featuring|such as)[:\\s]+([^.!?]{10,100})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in product_patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            # Split on common separators\n",
    "            items = re.split(r'[,;|&]', match)\n",
    "            for item in items[:5]:  # Limit to first 5 items\n",
    "                item = item.strip()\n",
    "                if len(item) > 3 and len(item) < 50:\n",
    "                    products.append(item.title())\n",
    "    \n",
    "    # Look for common product keywords\n",
    "    product_keywords = [\n",
    "        'software', 'app', 'platform', 'tool', 'service', 'solution',\n",
    "        'product', 'system', 'course', 'training', 'consulting',\n",
    "        'design', 'development', 'marketing', 'analytics'\n",
    "    ]\n",
    "    \n",
    "    words = content_lower.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in product_keywords and i > 0:\n",
    "            # Get context around the keyword\n",
    "            start = max(0, i-2)\n",
    "            end = min(len(words), i+3)\n",
    "            context = ' '.join(words[start:end])\n",
    "            if len(context) < 50:\n",
    "                products.append(context.title())\n",
    "    \n",
    "    # Remove duplicates and return top 5\n",
    "    unique_products = list(set(products))\n",
    "    return unique_products[:5] if unique_products else [\"Products and Services\"]\n",
    "\n",
    "print(extract_products(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7294107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General consumers and businesses\n"
     ]
    }
   ],
   "source": [
    "def extract_target_audience(content: str) -> str:\n",
    "    \"\"\"Extract target audience information\"\"\"\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Look for audience patterns\n",
    "    audience_patterns = [\n",
    "        r'(?:for|targeting|designed for|perfect for|ideal for)[:\\s]+([^.!?]{10,100})',\n",
    "        r'(?:customers?|clients?|users?|professionals?)[:\\s]+([^.!?]{10,100})',\n",
    "        r'(?:businesses?|companies?|individuals?|people who)[^.!?]*?([^.!?]{10,100})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in audience_patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE)\n",
    "        if matches:\n",
    "            audience = matches[0].strip()\n",
    "            if len(audience) > 10:\n",
    "                return audience.capitalize()\n",
    "    \n",
    "    # Look for demographic keywords\n",
    "    demo_keywords = {\n",
    "        'small business': 'Small business owners',\n",
    "        'enterprise': 'Enterprise clients',\n",
    "        'startup': 'Startups and entrepreneurs',\n",
    "        'professional': 'Working professionals',\n",
    "        'student': 'Students and learners',\n",
    "        'developer': 'Developers and technical teams',\n",
    "        'marketer': 'Marketing professionals',\n",
    "        'designer': 'Designers and creatives'\n",
    "    }\n",
    "    \n",
    "    for keyword, audience in demo_keywords.items():\n",
    "        if keyword in content_lower:\n",
    "            return audience\n",
    "    \n",
    "    return \"General consumers and businesses\"\n",
    "\n",
    "print(extract_target_audience(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aff4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High-quality products and services']\n"
     ]
    }
   ],
   "source": [
    "def extract_value_propositions(content: str) -> List[str]:\n",
    "    \"\"\"Extract key value propositions\"\"\"\n",
    "    \n",
    "    value_props = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Look for benefit patterns\n",
    "    benefit_patterns = [\n",
    "        r'(?:save|saves?|saving)[^.!?]*?([^.!?]{10,100}[.!?])',\n",
    "        r'(?:increase|increases?|boost|improve)[^.!?]*?([^.!?]{10,100}[.!?])',\n",
    "        r'(?:reduce|reduces?|eliminate|cut)[^.!?]*?([^.!?]{10,100}[.!?])',\n",
    "        r'(?:faster|quicker|easier|better)[^.!?]*?([^.!?]{10,100}[.!?])'\n",
    "    ]\n",
    "    \n",
    "    for pattern in benefit_patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE | re.DOTALL)\n",
    "        for match in matches[:3]:  # Limit to 3 per pattern\n",
    "            prop = match.strip()\n",
    "            if len(prop) > 15:\n",
    "                value_props.append(prop.capitalize())\n",
    "    \n",
    "    # Look for quality indicators\n",
    "    quality_keywords = [\n",
    "        'award-winning', 'industry-leading', 'best-in-class',\n",
    "        'proven', 'trusted', 'reliable', 'innovative', 'cutting-edge'\n",
    "    ]\n",
    "    \n",
    "    for keyword in quality_keywords:\n",
    "        if keyword in content_lower:\n",
    "            value_props.append(f\"{keyword.title()} solution\")\n",
    "    \n",
    "    return value_props[:5] if value_props else [\"High-quality products and services\"]\n",
    "\n",
    "print(extract_value_propositions(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8f5942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'smartest', 'fastest', 'most', 'useful', 'model', 'yet', 'built', 'thinking', 'puts', 'expert', 'level', 'intelligence', 'everyone', 'hands', 'learn']\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords(content: str) -> List[str]:\n",
    "    \"\"\"Extract relevant keywords from content\"\"\"\n",
    "    \n",
    "    # Common stop words to exclude\n",
    "    stop_words = {\n",
    "        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',\n",
    "        'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',\n",
    "        'after', 'above', 'below', 'between', 'among', 'this', 'that', 'these',\n",
    "        'those', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "        'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'\n",
    "    }\n",
    "    \n",
    "    # Clean and split content\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n",
    "    \n",
    "    # Count word frequency\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "    \n",
    "    # Sort by frequency and return top keywords\n",
    "    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    keywords = [word for word, count in sorted_words[:20] if count > 1]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "print(extract_keywords(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae2e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friendly\n"
     ]
    }
   ],
   "source": [
    "def analyze_brand_tone(content: str) -> str:\n",
    "    \"\"\"Analyze brand tone from content\"\"\"\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Tone indicators\n",
    "    formal_indicators = ['pursuant', 'therefore', 'hereby', 'whereas', 'furthermore']\n",
    "    casual_indicators = ['hey', 'awesome', 'cool', 'amazing', 'love', 'super']\n",
    "    professional_indicators = ['solution', 'expertise', 'professional', 'industry', 'enterprise']\n",
    "    friendly_indicators = ['welcome', 'help', 'support', 'team', 'together']\n",
    "    \n",
    "    scores = {\n",
    "        'formal': sum(1 for word in formal_indicators if word in content_lower),\n",
    "        'casual': sum(1 for word in casual_indicators if word in content_lower),\n",
    "        'professional': sum(1 for word in professional_indicators if word in content_lower),\n",
    "        'friendly': sum(1 for word in friendly_indicators if word in content_lower)\n",
    "    }\n",
    "    \n",
    "    # Determine dominant tone\n",
    "    max_tone = max(scores, key=scores.get)\n",
    "    \n",
    "    if scores[max_tone] == 0:\n",
    "        return \"Professional\"\n",
    "    \n",
    "    return max_tone.capitalize()\n",
    "\n",
    "print(analyze_brand_tone(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a633b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def extract_competitors(content: str) -> List[str]:\n",
    "    \"\"\"Extract potential competitors mentioned in content\"\"\"\n",
    "    \n",
    "    competitors = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Look for competitor patterns\n",
    "    competitor_patterns = [\n",
    "        r'(?:unlike|compared to|better than|vs\\.?|versus)[:\\s]+([A-Za-z0-9\\s]{3,30})',\n",
    "        r'(?:alternative to|competitor|competing with)[:\\s]+([A-Za-z0-9\\s]{3,30})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in competitor_patterns:\n",
    "        matches = re.findall(pattern, content_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            competitor = match.strip()\n",
    "            if len(competitor) > 2 and len(competitor) < 30:\n",
    "                competitors.append(competitor.title())\n",
    "    \n",
    "    return competitors[:5]\n",
    "\n",
    "\n",
    "print(extract_competitors(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeedc2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': '5\\nOur smartest'}\n"
     ]
    }
   ],
   "source": [
    "def extract_contact_info(content: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract contact information\"\"\"\n",
    "    \n",
    "    contact_info = {}\n",
    "    \n",
    "    # Email pattern\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, content)\n",
    "    if emails:\n",
    "        contact_info['email'] = emails[0]\n",
    "    \n",
    "    # Phone pattern\n",
    "    phone_pattern = r'(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})'\n",
    "    phones = re.findall(phone_pattern, content)\n",
    "    if phones:\n",
    "        contact_info['phone'] = phones[0]\n",
    "    \n",
    "    # Address pattern (basic)\n",
    "    address_patterns = [\n",
    "        r'(\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr))',\n",
    "        r'([A-Za-z\\s]+,\\s*[A-Z]{2}\\s+\\d{5})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in address_patterns:\n",
    "        addresses = re.findall(pattern, content, re.IGNORECASE)\n",
    "        if addresses:\n",
    "            contact_info['address'] = addresses[0]\n",
    "            break\n",
    "    \n",
    "    return contact_info\n",
    "\n",
    "print(extract_contact_info(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea6a29de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Service']\n"
     ]
    }
   ],
   "source": [
    "def extract_content_themes(content: str) -> List[str]:\n",
    "    \"\"\"Extract main content themes\"\"\"\n",
    "    \n",
    "    themes = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Theme categories\n",
    "    theme_keywords = {\n",
    "        'technology': ['software', 'tech', 'digital', 'ai', 'automation', 'platform'],\n",
    "        'health': ['health', 'wellness', 'medical', 'fitness', 'nutrition'],\n",
    "        'business': ['business', 'enterprise', 'corporate', 'b2b', 'professional'],\n",
    "        'education': ['education', 'learning', 'training', 'course', 'skill'],\n",
    "        'lifestyle': ['lifestyle', 'personal', 'home', 'family', 'life'],\n",
    "        'finance': ['finance', 'money', 'investment', 'financial', 'payment'],\n",
    "        'ecommerce': ['shop', 'store', 'buy', 'purchase', 'product', 'retail'],\n",
    "        'service': ['service', 'support', 'help', 'assistance', 'consultation']\n",
    "    }\n",
    "    \n",
    "    for theme, keywords in theme_keywords.items():\n",
    "        if any(keyword in content_lower for keyword in keywords):\n",
    "            themes.append(theme.title())\n",
    "    \n",
    "    return themes[:3] if themes else ['Business']\n",
    "\n",
    "print(extract_content_themes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae9fda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has_pricing': False, 'pricing_model': 'Unknown', 'price_points': []}\n"
     ]
    }
   ],
   "source": [
    "def extract_pricing_info(content: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract pricing information if available\"\"\"\n",
    "    \n",
    "    pricing_info = {\n",
    "        'has_pricing': False,\n",
    "        'pricing_model': 'Unknown',\n",
    "        'price_points': []\n",
    "    }\n",
    "    \n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    # Look for pricing indicators\n",
    "    pricing_keywords = ['price', 'pricing', 'cost', 'fee', 'subscription', 'plan']\n",
    "    if any(keyword in content_lower for keyword in pricing_keywords):\n",
    "        pricing_info['has_pricing'] = True\n",
    "    \n",
    "    # Price patterns\n",
    "    price_patterns = [\n",
    "        r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)',\n",
    "        r'(\\d+(?:,\\d{3})*(?:\\.\\d{2})?)\\s*(?:dollars?|usd)',\n",
    "        r'from\\s*\\$?(\\d+)',\n",
    "        r'starting\\s*at\\s*\\$?(\\d+)'\n",
    "    ]\n",
    "    \n",
    "    prices = []\n",
    "    for pattern in price_patterns:\n",
    "        matches = re.findall(pattern, content_lower)\n",
    "        prices.extend(matches)\n",
    "    \n",
    "    if prices:\n",
    "        pricing_info['price_points'] = [f\"${price}\" for price in prices[:5]]\n",
    "    \n",
    "    # Pricing model detection\n",
    "    if 'subscription' in content_lower or 'monthly' in content_lower:\n",
    "        pricing_info['pricing_model'] = 'Subscription'\n",
    "    elif 'one-time' in content_lower or 'lifetime' in content_lower:\n",
    "        pricing_info['pricing_model'] = 'One-time'\n",
    "    elif 'freemium' in content_lower or 'free trial' in content_lower:\n",
    "        pricing_info['pricing_model'] = 'Freemium'\n",
    "    \n",
    "    return pricing_info\n",
    "\n",
    "print(extract_pricing_info(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "898c7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def analyze_brand_from_url(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a brand's website to extract key information for marketing campaigns\n",
    "    \n",
    "    Args:\n",
    "        url: Brand website URL\n",
    "    \n",
    "    Returns:\n",
    "        Comprehensive brand analysis data\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get website content\n",
    "        website_content = get_website_text_content(url)\n",
    "        \n",
    "        if not website_content:\n",
    "            raise Exception(f\"Failed to extract content from {url}\")\n",
    "        \n",
    "        # Extract brand information\n",
    "        brand_analysis = {\n",
    "            \"url\": url,\n",
    "            \"brand_name\": extract_brand_name(website_content, url),\n",
    "            \"description\": extract_brand_description(website_content),\n",
    "            \"products\": extract_products(website_content),\n",
    "            \"target_audience\": extract_target_audience(website_content),\n",
    "            \"value_propositions\": extract_value_propositions(website_content),\n",
    "            \"keywords\": extract_keywords(website_content),\n",
    "            \"colors\": extract_brand_colors(url),\n",
    "            \"tone\": analyze_brand_tone(website_content),\n",
    "            \"competitors\": extract_competitors(website_content),\n",
    "            \"contact_info\": extract_contact_info(website_content),\n",
    "            \"social_links\": extract_social_links(url),\n",
    "            \"content_themes\": extract_content_themes(website_content),\n",
    "            \"pricing_info\": extract_pricing_info(website_content),\n",
    "            \"analyzed_at\": time.time()\n",
    "        }\n",
    "        \n",
    "        return brand_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"error\": str(e),\n",
    "            \"brand_name\": extract_domain_name(url),\n",
    "            \"description\": \"Brand analysis failed - using fallback data\",\n",
    "            \"products\": [\"Product or Service\"],\n",
    "            \"target_audience\": \"General consumers\",\n",
    "            \"analyzed_at\": time.time()\n",
    "        }\n",
    "output = analyze_brand_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d60aff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://openai.com', 'brand_name': 'Openai', 'description': 'Learn moreWhat can I help with?\\nOur smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.', 'products': ['Products and Services'], 'target_audience': 'General consumers and businesses', 'value_propositions': ['High-quality products and services'], 'keywords': ['our', 'smartest', 'fastest', 'most', 'useful', 'model', 'yet', 'built', 'thinking', 'puts', 'expert', 'level', 'intelligence', 'everyone', 'hands', 'learn'], 'colors': ['#007bff', '#6c757d'], 'tone': 'Friendly', 'competitors': [], 'contact_info': {'address': '5\\nOur smartest'}, 'social_links': {'twitter': 'https://twitter.com/OpenAI', 'instagram': 'https://instagram.com/openai', 'linkedin': 'https://linkedin.com/openai', 'tiktok': 'https://tiktok.com/openai'}, 'content_themes': ['Service'], 'pricing_info': {'has_pricing': False, 'pricing_model': 'Unknown', 'price_points': []}, 'analyzed_at': 1754691345.7661762}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c265e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://openai.com\n",
      "brand_name = Openai\n",
      "description = Learn moreWhat can I help with?\n",
      "Our smartest, fastest, most useful model yet, with built-in thinking that puts expert-level intelligence in everyone’s hands.\n",
      "products = ['Products and Services']\n",
      "target_audience = General consumers and businesses\n",
      "value_propositions = ['High-quality products and services']\n",
      "keywords = ['our', 'smartest', 'fastest', 'most', 'useful', 'model', 'yet', 'built', 'thinking', 'puts', 'expert', 'level', 'intelligence', 'everyone', 'hands', 'learn']\n",
      "colors = ['#007bff', '#6c757d']\n",
      "tone = Friendly\n",
      "competitors = []\n",
      "contact_info = {'address': '5\\nOur smartest'}\n",
      "social_links = {'twitter': 'https://twitter.com/OpenAI', 'instagram': 'https://instagram.com/openai', 'linkedin': 'https://linkedin.com/openai', 'tiktok': 'https://tiktok.com/openai'}\n",
      "content_themes = ['Service']\n",
      "pricing_info = {'has_pricing': False, 'pricing_model': 'Unknown', 'price_points': []}\n",
      "analyzed_at = 1754691345.7661762\n"
     ]
    }
   ],
   "source": [
    "for i, j in output.items():\n",
    "    print(i, \"=\", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7cfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
