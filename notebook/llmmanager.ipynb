{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go one level up from \"notebook/\" to the root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed82bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config Loaded:\n",
      "{'groq_api_key': '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', 'huggingface_api_key': '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', 'groq_model': 'llama3-8b-8192', 'huggingface_image_model': 'stabilityai/stable-diffusion-2-1', 'groq_api_url': 'https://api.groq.com/openai/v1/chat/completions', 'huggingface_api_url': 'https://api-inference.huggingface.co/models', 'max_tokens': 2048, 'temperature': 0.7, 'max_retries': 3, 'export_dir': 'export', 'memory_dir': 'memory', 'image_width': 1024, 'image_height': 1024, 'num_inference_steps': 20, 'max_emails': 10, 'max_sms': 5, 'max_ad_variants': 5, 'max_social_captions': 3}\n"
     ]
    }
   ],
   "source": [
    "# Now this will work:\n",
    "from config.settings import load_config, get_groq_headers\n",
    "# Load configuration settings\n",
    "config = load_config()\n",
    "print(\"âœ… Config Loaded:\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44dda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“œ LLM Response:\n",
      " Here is a short motivational quote:\n",
      "\n",
      "\"Believe in yourself, take the leap, and watch your dreams unfold. You are capable of achieving greatness, no matter what obstacles come your way. Keep pushing forward, stay focused, and never give up on your aspirations.\"\n"
     ]
    }
   ],
   "source": [
    "def get_llm_response(\n",
    "    prompt: str,\n",
    "    system_message: str = \"You are a helpful assistant.\",\n",
    "    model: str = None,\n",
    "    max_tokens: int = None,\n",
    "    temperature: float = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get response from Groq LLM API using Llama3-8B-8192 model\n",
    "    \n",
    "    Args:\n",
    "        prompt: User prompt/question\n",
    "        system_message: System context message\n",
    "        model: Model to use (defaults to config)\n",
    "        max_tokens: Max tokens to generate\n",
    "        temperature: Temperature for generation\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use config defaults if not specified\n",
    "    if model is None:\n",
    "        model = config[\"groq_model\"]\n",
    "    if max_tokens is None:\n",
    "        max_tokens = config[\"max_tokens\"]\n",
    "    if temperature is None:\n",
    "        temperature = config[\"temperature\"]\n",
    "    \n",
    "    # Prepare request payload\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Get headers\n",
    "    headers = get_groq_headers(config[\"groq_api_key\"])\n",
    "    \n",
    "    # Make API request with retries\n",
    "    for attempt in range(config[\"max_retries\"]):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                config[\"groq_api_url\"],\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                response_data = response.json()\n",
    "                \n",
    "                # Extract generated text\n",
    "                if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n",
    "                    content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    return content.strip()\n",
    "                else:\n",
    "                    raise Exception(\"No choices in response\")\n",
    "                    \n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                wait_time = 2 ** attempt\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == config[\"max_retries\"] - 1:\n",
    "                raise Exception(f\"Failed to get LLM response after {config['max_retries']} attempts: {str(e)}\")\n",
    "            \n",
    "            # Wait before retry\n",
    "            wait_time = 2 ** attempt\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    raise Exception(\"Failed to get LLM response\")\n",
    "\n",
    "\n",
    "prompt = \"Write a short motivational quote.\"\n",
    "response = get_llm_response(prompt)\n",
    "print(\"ðŸ“œ LLM Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06a47400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Contextual LLM Response:\n",
      " Here are a few tagline options for the Green Matcha Energy Drink targeting Gen Z:\n",
      "\n",
      "1. \"Level up your day, naturally.\"\n",
      "2. \"Unleash your green energy.\"\n",
      "3. \"Matcha made in heaven, fueled by you.\"\n",
      "4. \"Find your buzz, naturally.\"\n",
      "5. \"Greenify your grind.\"\n",
      "\n",
      "These taglines aim to resonate with Gen Z by using a mix of playful language, relatable references, and a focus on natural ingredients. The \"green\" in the product name is highlighted to appeal to the target audience's affinity for eco-friendly and sustainable products.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "def get_llm_response_with_context(\n",
    "    prompt: str,\n",
    "    context: Dict[str, Any],\n",
    "    system_message: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get LLM response with additional context information\n",
    "    \n",
    "    Args:\n",
    "        prompt: User prompt/question\n",
    "        context: Additional context information\n",
    "        system_message: System context message\n",
    "    \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build enhanced prompt with context\n",
    "    context_str = format_context_for_prompt(context)\n",
    "    enhanced_prompt = f\"{context_str}\\n\\n{prompt}\"\n",
    "    \n",
    "    return get_llm_response(enhanced_prompt, system_message)\n",
    "\n",
    "\n",
    "context = {\n",
    "    \"product_name\": \"Green Matcha Energy Drink\",\n",
    "    \"target_audience\": [\"Gen Z\"]\n",
    "}\n",
    "prompt = \"Write a product tagline.\"\n",
    "response = get_llm_response_with_context(prompt, context)\n",
    "print(\"ðŸ§  Contextual LLM Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b1c6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_prompt(context: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format context dictionary for inclusion in prompt\"\"\"\n",
    "    \n",
    "    context_lines = [\"Context Information:\"]\n",
    "    \n",
    "    for key, value in context.items():\n",
    "        if isinstance(value, (str, int, float)):\n",
    "            context_lines.append(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
    "        elif isinstance(value, list):\n",
    "            if value:  # Only add if list is not empty\n",
    "                context_lines.append(f\"- {key.replace('_', ' ').title()}: {', '.join(map(str, value))}\")\n",
    "        elif isinstance(value, dict):\n",
    "            context_lines.append(f\"- {key.replace('_', ' ').title()}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                context_lines.append(f\"  - {sub_key.replace('_', ' ').title()}: {sub_value}\")\n",
    "    \n",
    "    return '\\n'.join(context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2be31d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Authorization': 'Bearer dfsdfsdfsdfs', 'Content-Type': 'application/json'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "def get_huggingface_headers(api_key: str) -> Dict[str, str]:\n",
    "    \"\"\"Get headers for HuggingFace API requests\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "get_huggingface_headers('dfsdfsdfsdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52f045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
